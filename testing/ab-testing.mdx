---
title: "A/B Testing in Phonely"
description: "Description of your new file."
---

Phonely's A/B Testing tool helps you **experiment**, measure, and improve your voice AI's perfomance with real worl data . It allows you to compare different versions of you AI agent such as Voice, workflow, conversation settings to determine which one performs best in live calls.

This guide explains how A/B testing works, how to create and run a test, and how to interpret results.

## What is A/B Testing in Phonely ?

A/B Testing lets you run controlled experiments between two versions of your AI agent:

- The **Base Agent (Control)** — your exisiting setup.
- The **Test Agent (Variant)** — a duplicate with specific changes.

Incoming calls are automatically split between the two versions. Phonely then measures how each one perfoms based on the success criteria you define — such as call duration, outcomes, or end reasons.

This gives you clear, data-driven insights into what changes actually improve performance — instead of relying on guesswork.

## Where to Access A/B Testing

1. Go to your **Phonely Dashboard**.
2. Open your **Testing** tab.
3. Choose **A/B Testing** from the top navigation bar.

You’ll see three sections:

- **Planned** – Tests you’ve set up but haven’t started yet.
- **In Progress** – Tests currently running on live calls.
- **Completed** – Finished tests where you can review performance results.

## 3. Creating a New A/B Test

Click **Create a new test** in the _Planned_ section to begin. A step-by-step setup window will appear.

### **Step 1: Name and Describe Your Test**

- **Test Name:** Give your test a descriptive name that identifies what you’re testing.\
  _Example:_ “Friendly Voice vs Formal Voice – Support Line.”
- **Description:** Explain your test goal.\
  _Example:_ “Evaluate whether a friendly voice style improves appointment confirmations.”

This helps you keep track of multiple tests later

## Choose What You’d Like to Test

Phonely supports multiple types of tests depending on your experiment goal. You can select one of the following:

| Type               | What It Tests                                                    | Common Use Case                                                      |
| :----------------- | :--------------------------------------------------------------- | :------------------------------------------------------------------- |
| **Voice**          | Compares different AI voices or tones                            | Test if a friendly voice leads to higher customer engagement         |
| **Workflow**       | Tests different conversation flows or logic                      | Compare two calls scripts or routing paths                           |
| **Agent Settings** | Evaluates settings like interruption, delay, or background noise | Find the balance between quick responsed and natural flow            |
| **Knowledge Base** | Tests different documentation sources                            | See which knowlesdge sources improve the accuracy of the the answers |
| **Other**          | For any other tests outside thes categories                      |                                                                      |

Phonely supports multiple types of tests depending on your experiement goal. You can select one of the following:     

| Type               | What It Tests                                                    | Common Use Case                                              |
| :----------------- | :--------------------------------------------------------------- | :----------------------------------------------------------- |
| **Voice**          | Compares different AI voices or tones                            | Test if a friendly voice leads to higher customer engagement |
| **Workflow**       | Tests different conversation flows or logic                      | Compare two call scripts or routing paths                    |
| **Agent Settings** | Evaluates settings like interruption, delay, or background noise | Find the balance between quick responses and natural flow    |
| **Knowledge Base** | Tests different documentation sources                            | See which knowledge set improves answer accuracy             |
| **Other**          | For any test outside these categories                            | Experiment with custom or complex configurations             |

Once selected, click **Next**.

## Define End Criteria and Call Distribution

Here, you’ll specify **how long the test should run** and **what share of calls** should be routed to your test version.

### A. End Criteria

Choose when the test should stop automatically:

- **By Number of Calls:** Ends after a set number of test calls.\
  _Example:_ Stop after 1,000 calls routed to the test version.
- **By Number of Days:** Runs for a fixed duration (e.g., 10 days).
- **AI-Determined (Coming Soon):** Will allow Phonely to automatically decide when enough data is collected.

### B. Call Route Percentage

Use the slider to define how much traffic is sent to the test version.

- **Example:** Route 30% of calls to the test, and keep 70% on the base agent.
- **Recommendation:** Start small (20–30%) to ensure stability before scaling up.

After configuring both, click **Next**.

## 6. Set Success Criteria

This step defines what “success” means for your test. You can base success on how calls end, what outcomes are tagged, or how long they last.

### A. Call Ended Reason-Based Testing

Evaluates success based on how the call ended.\
Use this if you care about the _technical or behavioral outcome_ of the call.

- **Examples of End Reasons:**
  - Call Transferred.   
  - Voicemail
  - Max Duration
  - Silence Timeout
  - Customer Ended
  - Agent Ended

_Use case:_ “We want more calls to end in transfers to the sales team.”

### B. Call Outcome-Based Testing

Evaluates based on your defined **business outcomes** — which you can configure inside your flow.

- **Examples:**
  - Appointment Booked
  - Lead Qualified
  - Legal Inquiry Logged
  - Support Issue Resolved

_Use case:_ “We want to see if the new workflow increases lead qualification rate.”

### C. Duration-Based Testing

Optimizes for call length.

- **Shorter Calls:** Indicates more efficency or faster resolution (ideal for support or routing).
- **Longer Calls:** Indicates better engagement or deeper discussions (ideal for sales).

_Use case:_ “Does the new prompt shorten average support calls by 15%?”

### D. LLM-Based Evaluation (Coming Soon)

A future option will allow Phonely’s AI to analyze transcripts and automatically evaluate call quality based on context.

Once you’ve chosen and configured your success criteria, click **Next**.

## 7. Editing the Test Agent

After setup, Phonely automatically duplicates your base agent into a **Test Agent**.\
You’ll see a banner:

> “You are editing a test agent. This agent will be used to test the new changes.”

You can now modify only what you want to test:

- Change the voice  or personality for voice tests. 
- Adjust workflow blocks or logic paths for workflow tests.
- Update agent settings such as response delay or background noise for settings tests.
- Modify knowledge sources for knowledge base tests.

Keep all other elements identical to ensure that results reflect only the changes you made.

Once done, click **Continue** to save your test agent.

## 8. Running the A/B Test

After your setup is complete, you’ll return to the A/B Testing dashboard.

1. Under the **Planned** section, find your new test.
2. Click **Begin Test** to start routing calls.

Your test will then appear under **In Progress**, showing live metrics such as:

- **Success Rate** for each agent
- **Total Answered Calls**
- **Traffic Allocation**

Calls will automatically be divided between your Base Agent and Test Agent.

## 9. Monitoring and Analyzing Results

You can monitor ongoing results anytime during the test:

- Track **Success Rate Trends** to see which variant performs better.
- Check if call allocation percentages remain balanced.
- Review **call outcomes** and **end reasons** to ensure tagging consistency.

Once your call limit or duration target is reached, the test moves to the **Completed** section.

Click **View Results** to analyze:

- Performance metrics (success rate, duration, end reason distribution).
- Comparative insights between Base and Test agents.
- Which version achieved better alignment with your success criteria.

The variant with the higher success percentage is your **winning configuration** — which you can apply to your main agent for future calls.